{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Estatística\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes Paramétricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa realizamos os testes de normalidade de Shapiro-Wilk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados do teste de Shapiro-Wilk:\n",
      "      Metric     Classifier  Statistic  P-value  Is Normal\n",
      "0   Accuracy            KNN     0.9674   0.8660       True\n",
      "1   Accuracy  Decision Tree     0.9226   0.3789       True\n",
      "2   Accuracy  Random Forest     0.9513   0.6835       True\n",
      "3   Accuracy        XGBoost     0.9200   0.3566       True\n",
      "4   Accuracy       LightGBM     0.9017   0.2289       True\n",
      "5   F1 Score            KNN     0.9664   0.8559       True\n",
      "6   F1 Score  Decision Tree     0.9171   0.3331       True\n",
      "7   F1 Score  Random Forest     0.9553   0.7316       True\n",
      "8   F1 Score        XGBoost     0.9203   0.3591       True\n",
      "9   F1 Score       LightGBM     0.9005   0.2220       True\n",
      "10    Recall            KNN     0.9542   0.7187       True\n",
      "11    Recall  Decision Tree     0.8725   0.1069       True\n",
      "12    Recall  Random Forest     0.9484   0.6498       True\n",
      "13    Recall        XGBoost     0.9752   0.9348       True\n",
      "14    Recall       LightGBM     0.8781   0.1241       True\n",
      "15      ACSA            KNN     0.9442   0.6002       True\n",
      "16      ACSA  Decision Tree     0.9280   0.4283       True\n",
      "17      ACSA  Random Forest     0.9343   0.4916       True\n",
      "18      ACSA        XGBoost     0.9436   0.5934       True\n",
      "19      ACSA       LightGBM     0.9230   0.3828       True\n",
      "\n",
      "Resumo da normalidade por métrica:\n",
      "\n",
      "Accuracy:\n",
      "- Distribuições normais: 5 de 5 classificadores\n",
      "- Classificadores com distribuição normal:\n",
      "  * KNN\n",
      "  * Decision Tree\n",
      "  * Random Forest\n",
      "  * XGBoost\n",
      "  * LightGBM\n",
      "\n",
      "F1 Score:\n",
      "- Distribuições normais: 5 de 5 classificadores\n",
      "- Classificadores com distribuição normal:\n",
      "  * KNN\n",
      "  * Decision Tree\n",
      "  * Random Forest\n",
      "  * XGBoost\n",
      "  * LightGBM\n",
      "\n",
      "Recall:\n",
      "- Distribuições normais: 5 de 5 classificadores\n",
      "- Classificadores com distribuição normal:\n",
      "  * KNN\n",
      "  * Decision Tree\n",
      "  * Random Forest\n",
      "  * XGBoost\n",
      "  * LightGBM\n",
      "\n",
      "ACSA:\n",
      "- Distribuições normais: 5 de 5 classificadores\n",
      "- Classificadores com distribuição normal:\n",
      "  * KNN\n",
      "  * Decision Tree\n",
      "  * Random Forest\n",
      "  * XGBoost\n",
      "  * LightGBM\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Carregar arquivo Excel\n",
    "excel_file_path = 'metrics_results.xlsx'\n",
    "excel_data = pd.ExcelFile(excel_file_path)\n",
    "\n",
    "# Dicionário para armazenar os valores extraídos\n",
    "extracted_values = {}\n",
    "\n",
    "# Ler dados de cada aba e extrair as 10 primeiras linhas de cada classificador\n",
    "for sheet_name in excel_data.sheet_names:\n",
    "    sheet_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "    classifiers = ['KNN', 'Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM']\n",
    "    for classifier in classifiers:\n",
    "        extracted_values[f\"{sheet_name}_{classifier}\"] = sheet_df[classifier].iloc[:10].tolist()\n",
    "\n",
    "# Realizar teste de Shapiro-Wilk para cada métrica e classificador\n",
    "shapiro_results = {}\n",
    "\n",
    "for metric in ['Accuracy', 'F1 Score', 'Recall', 'ACSA']:\n",
    "    shapiro_results[metric] = {}\n",
    "    for classifier in classifiers:\n",
    "        # Obter os dados\n",
    "        data = extracted_values[f\"{metric}_{classifier}\"]\n",
    "        \n",
    "        # Realizar o teste de Shapiro-Wilk\n",
    "        statistic, p_value = stats.shapiro(data)\n",
    "        \n",
    "        # Armazenar resultados\n",
    "        shapiro_results[metric][classifier] = {\n",
    "            'Statistic': statistic,\n",
    "            'P-value': p_value,\n",
    "            'Is Normal': p_value > 0.05  # Considerando alfa = 0.05\n",
    "        }\n",
    "\n",
    "# Criar DataFrame com os resultados\n",
    "results_data = []\n",
    "for metric in shapiro_results:\n",
    "    for classifier in shapiro_results[metric]:\n",
    "        results_data.append({\n",
    "            'Metric': metric,\n",
    "            'Classifier': classifier,\n",
    "            'Statistic': shapiro_results[metric][classifier]['Statistic'],\n",
    "            'P-value': shapiro_results[metric][classifier]['P-value'],\n",
    "            'Is Normal': shapiro_results[metric][classifier]['Is Normal']\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Formatar o DataFrame para melhor visualização\n",
    "results_df['Statistic'] = results_df['Statistic'].round(4)\n",
    "results_df['P-value'] = results_df['P-value'].round(4)\n",
    "\n",
    "# Salvar resultados em CSV\n",
    "results_df.to_csv('shapiro_wilk_results.csv', index=False)\n",
    "print(\"Resultados do teste de Shapiro-Wilk:\")\n",
    "print(results_df)\n",
    "\n",
    "# Criar um resumo por métrica\n",
    "print(\"\\nResumo da normalidade por métrica:\")\n",
    "for metric in ['Accuracy', 'F1 Score', 'Recall', 'ACSA']:\n",
    "    normal_count = len(results_df[(results_df['Metric'] == metric) & (results_df['Is Normal'])])\n",
    "    total_count = len(results_df[results_df['Metric'] == metric])\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"- Distribuições normais: {normal_count} de {total_count} classificadores\")\n",
    "    print(\"- Classificadores com distribuição normal:\")\n",
    "    normal_classifiers = results_df[(results_df['Metric'] == metric) & (results_df['Is Normal'])]['Classifier'].tolist()\n",
    "    if normal_classifiers:\n",
    "        for clf in normal_classifiers:\n",
    "            print(f\"  * {clf}\")\n",
    "    else:\n",
    "        print(\"  * Nenhum classificador apresentou distribuição normal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes Não Paramétricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa realizamos os teste de Friedman, post hoc Niemenyi e mapas de calor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados do teste de Nemenyi para Accuracy exportados para 'nemenyi_results_Accuracy.csv'.\n",
      "Mapa de calor para Accuracy salvo como 'nemenyi_heatmap_Accuracy.png'.\n",
      "Resultados do teste de Nemenyi para F1 Score exportados para 'nemenyi_results_F1 Score.csv'.\n",
      "Mapa de calor para F1 Score salvo como 'nemenyi_heatmap_F1 Score.png'.\n",
      "Resultados do teste de Nemenyi para Recall exportados para 'nemenyi_results_Recall.csv'.\n",
      "Mapa de calor para Recall salvo como 'nemenyi_heatmap_Recall.png'.\n",
      "Resultados do teste de Nemenyi para ACSA exportados para 'nemenyi_results_ACSA.csv'.\n",
      "Mapa de calor para ACSA salvo como 'nemenyi_heatmap_ACSA.png'.\n",
      "\n",
      "Resultados do teste de Friedman:\n",
      "          Statistic       P-value\n",
      "Accuracy      38.08  1.078775e-07\n",
      "F1 Score      38.08  1.078775e-07\n",
      "Recall        38.56  8.587571e-08\n",
      "ACSA          38.00  1.120559e-07\n"
     ]
    }
   ],
   "source": [
    "import scikit_posthocs as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import friedmanchisquare\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carregar arquivo Excel\n",
    "excel_file_path = 'metrics_results.xlsx'\n",
    "excel_data = pd.ExcelFile(excel_file_path)\n",
    "\n",
    "# Dicionário para armazenar os valores extraídos\n",
    "extracted_values = {}\n",
    "\n",
    "# Ler dados de cada aba e extrair as 10 primeiras linhas de cada classificador\n",
    "for sheet_name in excel_data.sheet_names:\n",
    "    sheet_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "    classifiers = ['KNN', 'Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM']\n",
    "    for classifier in classifiers:\n",
    "        extracted_values[f\"{sheet_name}_{classifier}\"] = sheet_df[classifier].iloc[:10].tolist()\n",
    "\n",
    "# Dicionário com as métricas e seus respectivos valores\n",
    "metrics = {\n",
    "    \"Accuracy\": [extracted_values['Accuracy_KNN'], \n",
    "                 extracted_values['Accuracy_Decision Tree'],\n",
    "                 extracted_values['Accuracy_Random Forest'],\n",
    "                 extracted_values['Accuracy_XGBoost'],\n",
    "                 extracted_values['Accuracy_LightGBM']],\n",
    "    \n",
    "    \"F1 Score\": [extracted_values['F1 Score_KNN'],\n",
    "                 extracted_values['F1 Score_Decision Tree'],\n",
    "                 extracted_values['F1 Score_Random Forest'],\n",
    "                 extracted_values['F1 Score_XGBoost'],\n",
    "                 extracted_values['F1 Score_LightGBM']],\n",
    "    \n",
    "    \"Recall\": [extracted_values['Recall_KNN'],\n",
    "               extracted_values['Recall_Decision Tree'],\n",
    "               extracted_values['Recall_Random Forest'],\n",
    "               extracted_values['Recall_XGBoost'],\n",
    "               extracted_values['Recall_LightGBM']],\n",
    "    \n",
    "    \"ACSA\": [extracted_values['ACSA_KNN'],\n",
    "             extracted_values['ACSA_Decision Tree'],\n",
    "             extracted_values['ACSA_Random Forest'],\n",
    "             extracted_values['ACSA_XGBoost'],\n",
    "             extracted_values['ACSA_LightGBM']]\n",
    "}\n",
    "\n",
    "# Nomes dos classificadores\n",
    "classifiers = ['KNN', 'Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM']\n",
    "\n",
    "# Primeiro executar o teste de Friedman para cada métrica\n",
    "friedman_results = {}\n",
    "for metric, data in metrics.items():\n",
    "    stat, p_value = friedmanchisquare(*data)\n",
    "    friedman_results[metric] = {\"Statistic\": stat, \"P-value\": p_value}\n",
    "\n",
    "# Executar o teste pós-hoc de Nemenyi para métricas significativas\n",
    "posthoc_results = {}\n",
    "for metric, data in metrics.items():\n",
    "    # Verificar se o teste de Friedman indicou diferença significativa\n",
    "    if friedman_results[metric][\"P-value\"] < 0.05:\n",
    "        data_transposed = np.array(data).T  # Transpor os dados\n",
    "        nemenyi = sp.posthoc_nemenyi_friedman(data_transposed)\n",
    "        nemenyi.index = classifiers\n",
    "        nemenyi.columns = classifiers\n",
    "        posthoc_results[metric] = nemenyi\n",
    "\n",
    "        # Exportar resultados do teste de Nemenyi para CSV\n",
    "        nemenyi.to_csv(f'nemenyi_results_{metric}.csv')\n",
    "        print(f\"Resultados do teste de Nemenyi para {metric} exportados para 'nemenyi_results_{metric}.csv'.\")\n",
    "\n",
    "        # Criar e salvar o heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            nemenyi,\n",
    "            annot=True,\n",
    "            fmt=\".3f\",\n",
    "            cmap=\"coolwarm\",\n",
    "            cbar_kws={'label': 'Valor-p'},\n",
    "            xticklabels=nemenyi.columns,\n",
    "            yticklabels=nemenyi.index\n",
    "        )\n",
    "        plt.title(f'Mapa de Calor - Teste de Nemenyi ({metric})', fontsize=16)\n",
    "        plt.xlabel('Classificadores', fontsize=12)\n",
    "        plt.ylabel('Classificadores', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Salvar mapa de calor como imagem\n",
    "        heatmap_path = f'nemenyi_heatmap_{metric}.png'\n",
    "        plt.savefig(heatmap_path)\n",
    "        print(f\"Mapa de calor para {metric} salvo como '{heatmap_path}'.\")\n",
    "        plt.close()  # Fechar a figura para liberar memória\n",
    "\n",
    "# Mostrar resultados do teste de Friedman\n",
    "print(\"\\nResultados do teste de Friedman:\")\n",
    "friedman_df = pd.DataFrame.from_dict(friedman_results, orient='index')\n",
    "print(friedman_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Média e Desvio Padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'metricas_estatisticas.xlsx' foi criado com sucesso!\n",
      "\n",
      "Aba 1 - Accuracy e F1 Score:\n",
      "   Classificador  Média Accuracy  Desvio Padrão Accuracy  Média F1 Score  \\\n",
      "0            KNN          0.9766                  0.0003          0.9763   \n",
      "1  Decision Tree          0.9737                  0.0009          0.9735   \n",
      "2  Random Forest          0.9581                  0.0011          0.9568   \n",
      "3        XGBoost          0.9924                  0.0004          0.9925   \n",
      "4       LightGBM          0.9926                  0.0005          0.9927   \n",
      "\n",
      "   Desvio Padrão F1 Score  \n",
      "0                  0.0003  \n",
      "1                  0.0009  \n",
      "2                  0.0012  \n",
      "3                  0.0004  \n",
      "4                  0.0005  \n",
      "\n",
      "Aba 2 - Recall e ACSA:\n",
      "   Classificador  Média Recall  Desvio Padrão Recall  Média ACSA  \\\n",
      "0            KNN        0.9545                0.0005      0.9979   \n",
      "1  Decision Tree        0.9574                0.0017      0.9819   \n",
      "2  Random Forest        0.9221                0.0024      0.9627   \n",
      "3        XGBoost        0.9893                0.0006      0.9993   \n",
      "4       LightGBM        0.9888                0.0007      0.9992   \n",
      "\n",
      "   Desvio Padrão ACSA  \n",
      "0              0.0003  \n",
      "1              0.0007  \n",
      "2              0.0011  \n",
      "3              0.0001  \n",
      "4              0.0001  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Carregar arquivo Excel\n",
    "excel_file_path = 'metrics_results.xlsx'\n",
    "excel_data = pd.ExcelFile(excel_file_path)\n",
    "\n",
    "# Dicionário para armazenar os valores extraídos\n",
    "extracted_values = {}\n",
    "\n",
    "# Ler dados de cada aba e extrair as 10 primeiras linhas de cada classificador\n",
    "for sheet_name in excel_data.sheet_names:\n",
    "    sheet_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "    classifiers = ['KNN', 'Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM']\n",
    "    for classifier in classifiers:\n",
    "        extracted_values[f\"{sheet_name}_{classifier}\"] = sheet_df[classifier].iloc[:10].tolist()\n",
    "\n",
    "# Calcular estatísticas para cada métrica e classificador\n",
    "stats_results = {\n",
    "    'Accuracy_F1': [],\n",
    "    'Recall_ACSA': []\n",
    "}\n",
    "\n",
    "# Lista de classificadores\n",
    "classifiers = ['KNN', 'Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM']\n",
    "\n",
    "# Calcular estatísticas para Accuracy e F1 Score\n",
    "for classifier in classifiers:\n",
    "    accuracy_data = extracted_values[f\"Accuracy_{classifier}\"]\n",
    "    f1_data = extracted_values[f\"F1 Score_{classifier}\"]\n",
    "    \n",
    "    stats_results['Accuracy_F1'].append({\n",
    "        'Classificador': classifier,\n",
    "        'Média Accuracy': np.mean(accuracy_data),\n",
    "        'Desvio Padrão Accuracy': np.std(accuracy_data),\n",
    "        'Média F1 Score': np.mean(f1_data),\n",
    "        'Desvio Padrão F1 Score': np.std(f1_data)\n",
    "    })\n",
    "\n",
    "# Calcular estatísticas para Recall e ACSA\n",
    "for classifier in classifiers:\n",
    "    recall_data = extracted_values[f\"Recall_{classifier}\"]\n",
    "    acsa_data = extracted_values[f\"ACSA_{classifier}\"]\n",
    "    \n",
    "    stats_results['Recall_ACSA'].append({\n",
    "        'Classificador': classifier,\n",
    "        'Média Recall': np.mean(recall_data),\n",
    "        'Desvio Padrão Recall': np.std(recall_data),\n",
    "        'Média ACSA': np.mean(acsa_data),\n",
    "        'Desvio Padrão ACSA': np.std(acsa_data)\n",
    "    })\n",
    "\n",
    "# Criar DataFrames\n",
    "df_accuracy_f1 = pd.DataFrame(stats_results['Accuracy_F1'])\n",
    "df_recall_acsa = pd.DataFrame(stats_results['Recall_ACSA'])\n",
    "\n",
    "# Arredondar valores para 4 casas decimais\n",
    "columns_to_round = [col for col in df_accuracy_f1.columns if col != 'Classificador']\n",
    "df_accuracy_f1[columns_to_round] = df_accuracy_f1[columns_to_round].round(4)\n",
    "\n",
    "columns_to_round = [col for col in df_recall_acsa.columns if col != 'Classificador']\n",
    "df_recall_acsa[columns_to_round] = df_recall_acsa[columns_to_round].round(4)\n",
    "\n",
    "# Criar arquivo Excel com as duas abas\n",
    "with pd.ExcelWriter('metricas_estatisticas.xlsx') as writer:\n",
    "    df_accuracy_f1.to_excel(writer, sheet_name='Accuracy e F1 Score', index=False)\n",
    "    df_recall_acsa.to_excel(writer, sheet_name='Recall e ACSA', index=False)\n",
    "\n",
    "print(\"Arquivo 'metricas_estatisticas.xlsx' foi criado com sucesso!\")\n",
    "print(\"\\nAba 1 - Accuracy e F1 Score:\")\n",
    "print(df_accuracy_f1)\n",
    "print(\"\\nAba 2 - Recall e ACSA:\")\n",
    "print(df_recall_acsa)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
