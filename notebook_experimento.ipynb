{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr1v1FVY3fYe"
   },
   "source": [
    "## Importação e Preparação da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "YXIEaYpQXcOL"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install scikit-optimize\n",
    "# !pip install openml\n",
    "# !pip install optuna\n",
    "# !pip install neupy\n",
    "# !pip install --upgrade neupy\n",
    "# !pip install --upgrade theano\n",
    "# !pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente local detectado.\n",
      "'scikit-optimize' já está instalado: versão 0.10.2\n",
      "'openml' já está instalado: versão 0.15.0\n",
      "'optuna' já está instalado: versão 4.1.0\n",
      "'imbalanced-learn' já está instalado: versão 0.12.4\n",
      "'openpyxl' já está instalado: versão 3.1.5\n",
      "Instalando 'scikit-posthocs'...\n",
      "Atualizando 'neupy' e 'theano' no ambiente local...\n",
      "Requirement already satisfied: neupy in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (0.6.5)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting neupy\n",
      "  Obtaining dependency information for neupy from https://files.pythonhosted.org/packages/21/be/19082cbe9a6c76dd909255341587f0b487cd3e9d32d44debda013d2accd1/neupy-0.8.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached neupy-0.8.2-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from neupy) (2.1.3)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from neupy) (1.14.1)\n",
      "INFO: pip is looking at multiple versions of neupy to determine which version is compatible with other requirements. This could take a while.\n",
      "  Obtaining dependency information for neupy from https://files.pythonhosted.org/packages/c9/da/fec21f526864b850dea52a9d09e633d303c78588aca3af00a314e41960a4/neupy-0.8.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached neupy-0.8.1-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "  Obtaining dependency information for neupy from https://files.pythonhosted.org/packages/9c/a2/4d8dc5d686adcdd3ed2c98c85ccf69370ef0675574b0f8dbb9b91abc978c/neupy-0.8.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached neupy-0.8.0-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "  Obtaining dependency information for neupy from https://files.pythonhosted.org/packages/69/5e/13197fd26b5bf09cf3ae08e18c0bfb26407a96757c2f60651e4b52ede7ab/neupy-0.7.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached neupy-0.7.3-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "  Obtaining dependency information for neupy from https://files.pythonhosted.org/packages/b1/1d/667a0ffc23441dbebdecadaa7076adc0544c09e9975b25b38d55e8d66fc4/neupy-0.7.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached neupy-0.7.2-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "  Obtaining dependency information for neupy from https://files.pythonhosted.org/packages/80/d1/09c645feff9917d53889f2256207ba9fba11681b4fcb8326e95b776edc51/neupy-0.7.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached neupy-0.7.1-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "  Obtaining dependency information for neupy from https://files.pythonhosted.org/packages/6a/9d/ccbe1f517596a7c91108ebb7dff84551a6f7c9ccf601c0569d3c2bd92136/neupy-0.7.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached neupy-0.7.0-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting Theano==1.0.0 (from neupy)\n",
      "  Using cached Theano-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib>=1.5.1 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from neupy) (3.9.2)\n",
      "Requirement already satisfied: graphviz==0.5.1 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from neupy) (0.5.1)\n",
      "Requirement already satisfied: tableprint==0.7.1 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from neupy) (0.7.1)\n",
      "Requirement already satisfied: progressbar2==3.34.3 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from neupy) (3.34.3)\n",
      "Requirement already satisfied: python-utils>=2.1.0 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from progressbar2==3.34.3->neupy) (3.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from tableprint==0.7.1->neupy) (1.16.0)\n",
      "Requirement already satisfied: future in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from tableprint==0.7.1->neupy) (1.0.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from tableprint==0.7.1->neupy) (0.2.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from matplotlib>=1.5.1->neupy) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from matplotlib>=1.5.1->neupy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from matplotlib>=1.5.1->neupy) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from matplotlib>=1.5.1->neupy) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from matplotlib>=1.5.1->neupy) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from matplotlib>=1.5.1->neupy) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from matplotlib>=1.5.1->neupy) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from matplotlib>=1.5.1->neupy) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>3.10.0.2 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from python-utils>=2.1.0->progressbar2==3.34.3->neupy) (4.12.2)\n",
      "Installing collected packages: Theano\n",
      "  Attempting uninstall: Theano\n",
      "    Found existing installation: Theano 1.0.5\n",
      "    Uninstalling Theano-1.0.5:\n",
      "      Successfully uninstalled Theano-1.0.5\n",
      "Successfully installed Theano-1.0.0\n",
      "Requirement already satisfied: theano in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (1.0.0)\n",
      "Collecting theano\n",
      "  Using cached Theano-1.0.5-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from theano) (2.1.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from theano) (1.14.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\willi\\documents\\projetos\\mestrado_mineracao\\mda\\lib\\site-packages (from theano) (1.16.0)\n",
      "Installing collected packages: theano\n",
      "  Attempting uninstall: theano\n",
      "    Found existing installation: Theano 1.0.0\n",
      "    Uninstalling Theano-1.0.0:\n",
      "      Successfully uninstalled Theano-1.0.0\n",
      "Successfully installed theano-1.0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "neupy 0.6.5 requires Theano==1.0.0, but you have theano 1.0.5 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Verificação de pacotes\n",
    "#%%capture\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "# Função para instalar pacotes, se necessário\n",
    "def install_package(package):\n",
    "    try:\n",
    "        # Verificar se o pacote já está instalado\n",
    "        dist = pkg_resources.get_distribution(package)\n",
    "        print(f\"'{package}' já está instalado: versão {dist.version}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        # Instalar o pacote, se não estiver instalado\n",
    "        print(f\"Instalando '{package}'...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Lista de pacotes necessários\n",
    "packages = [\n",
    "    \"scikit-optimize\",\n",
    "    \"openml\",\n",
    "    \"optuna\",\n",
    "    \"neupy\",\n",
    "    \"theano\",\n",
    "    \"imbalanced-learn\",\n",
    "    \"openpyxl\",\n",
    "    \"scikit-posthocs\"\n",
    "]\n",
    "\n",
    "# Verificar se está no Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    is_colab = True\n",
    "    print(\"Detectado ambiente Google Colab.\")\n",
    "except ImportError:\n",
    "    is_colab = False\n",
    "    print(\"Ambiente local detectado.\")\n",
    "\n",
    "# Instalar pacotes\n",
    "for package in packages:\n",
    "    if is_colab or package not in [\"neupy\", \"theano\"]:  # 'neupy' e 'theano' apenas em ambiente local\n",
    "        install_package(package)\n",
    "\n",
    "# Atualizar pacotes específicos\n",
    "if not is_colab:\n",
    "    print(\"Atualizando 'neupy' e 'theano' no ambiente local...\")\n",
    "    # Install the latest version of NeuPy#+\n",
    "    !pip install --upgrade neupy\n",
    "    # Install the required package\n",
    "    !pip install --upgrade theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VLgev7DN7Fxj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import openml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IVNblnZP60uV"
   },
   "outputs": [],
   "source": [
    "# Definir o ID do dataset no OpenML\n",
    "dataset_id = 722\n",
    "\n",
    "# Carregar o dataset diretamente do OpenML\n",
    "dataset = openml.datasets.get_dataset(dataset_id)\n",
    "data = dataset.get_data()[0]  # Pega o DataFrame completo\n",
    "\n",
    "# Remove colunas onde todos os valores são zero: [f10-f12] e [f34-f48]\n",
    "data = data.loc[:, (data != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "U_utNjnbIYd4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f13', 'f14',\n",
       "       'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24',\n",
       "       'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33',\n",
       "       'binaryClass'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85poJhhVSYzF"
   },
   "source": [
    "## Utilizando ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PDNh3_lUSb1E"
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import pandas as pd\n",
    "\n",
    "# Definir a variável alvo\n",
    "target_column = 'binaryClass'\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Supondo que y tenha as classes 'N' e 'P' como strings\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  # Converte as classes para números (0 e 1)\n",
    "\n",
    "\n",
    "# Dividir os dados em treino e teste com estratificação\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Definir o StratifiedKFold com 10 folds para garantir a estratificação nas validações cruzadas\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Identificar colunas numéricas e ajustar o tipo de dado para float64\n",
    "numeric_features = X_train.select_dtypes(include=['uint8']).columns\n",
    "X_train[numeric_features] = X_train[numeric_features].astype('float64')\n",
    "X_test[numeric_features] = X_test[numeric_features].astype('float64')\n",
    "\n",
    "# Substituir valores ausentes pela média em cada coluna numérica\n",
    "X_train[numeric_features] = X_train[numeric_features].fillna(X_train[numeric_features].mean())\n",
    "X_test[numeric_features] = X_test[numeric_features].fillna(X_train[numeric_features].mean())\n",
    "\n",
    "# Aplicar transformação (padronização) nas colunas numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "# Aplicar ADASYN para balancear as classes no conjunto de treino\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Agora X_train_resampled e y_train_resampled estão prontos para uso em modelos com 10-fold cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgg2SmPTPfVj"
   },
   "source": [
    "## Início experimento - Variáveis Básicas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O-eiuwm5PiD5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\Documents\\Projetos\\mestrado_mineracao\\mda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Realizando experimento já considerando X_train_resampled e y_train_resampled\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical, Real\n",
    "# from neupy import algorithms\n",
    "\n",
    "# Configurar K-Fold com estratificação\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Definição básica dos classificadores\n",
    "knn = KNeighborsClassifier()\n",
    "lvq = MLPClassifier(solver='sgd', learning_rate='constant', learning_rate_init=0.1, max_iter=100)\n",
    "tree = DecisionTreeClassifier()\n",
    "svm = SVC(probability=True)\n",
    "rf = RandomForestClassifier()\n",
    "mlp = MLPClassifier(max_iter=100, hidden_layer_sizes=(50,))\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "lgbm = LGBMClassifier(n_estimators=50, max_depth=3)\n",
    "\n",
    "# Comitê de Redes Neurais Artificiais\n",
    "ann_ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('mlp_relu', MLPClassifier(activation='relu', hidden_layer_sizes=(50, 20), max_iter=100)),\n",
    "        ('mlp_tanh', MLPClassifier(activation='tanh', hidden_layer_sizes=(50, 20), max_iter=100)),\n",
    "        ('mlp_logistic', MLPClassifier(activation='logistic', hidden_layer_sizes=(50, 20), max_iter=100))\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1  # Paralelização\n",
    ")\n",
    "\n",
    "# Comitê Heterogêneo (Stacking)\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('nb', GaussianNB()),  # Modelo rápido e leve\n",
    "        ('dt', DecisionTreeClassifier(max_depth=3)),  # Árvore de decisão rasa\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=100),  # Meta-modelo simples\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Dicionário de classificadores inicial:\n",
    "classifiers = {\n",
    "    'KNN': knn,             # Bayesian Search\n",
    "    'SVM': svm,\n",
    "    'Decision Tree': tree,  # Bayesian Search\n",
    "    'LVQ': lvq,\n",
    "    'MLP': mlp,\n",
    "    'Ensemble Neural Network': ann_ensemble,\n",
    "    'Stacking': stacking,\n",
    "    'Random Forest': rf,    #Optuna\n",
    "    'XGBoost': xgb,         #Optuna\n",
    "    'LightGBM': lgbm\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAz_-U69MqPw"
   },
   "source": [
    "## Tunning Usando Bayesian e Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3ngnvnnMtzQ"
   },
   "outputs": [],
   "source": [
    "# Usando BS e Optuna em dois classificadores diferentes:\n",
    "\n",
    "# Filtrar warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger('sklearn').setLevel(logging.ERROR)\n",
    "\n",
    "# Configurações para os modelos que costumam gerar warnings\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "# Configurações específicas para XGBoost\n",
    "from xgboost import set_config\n",
    "set_config(verbosity=0)\n",
    "\n",
    "# Espaços de busca para Bayesian Search: KNN e Decision Tree\n",
    "param_spaces = {\n",
    "    'KNN': {\n",
    "        'n_neighbors': (1, 30),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': (1, 2)\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': (1, 50),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'min_samples_leaf': (1, 20),\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "# Avaliar classificadores e buscar hiperparâmetros com BayesSearchCV\n",
    "results = {}\n",
    "best_params = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    if name in param_spaces:\n",
    "        print(f\"Otimizando hiperparâmetros para {name}...\")\n",
    "        bayes_search = BayesSearchCV(\n",
    "            estimator=clf,\n",
    "            search_spaces=param_spaces[name],\n",
    "            n_iter=50,\n",
    "            cv=kfold,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Garante que X_train_resampled seja um DataFrame com os nomes de coluna corretos\n",
    "        X_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
    "\n",
    "        bayes_search.fit(X_train_resampled, y_train_resampled)\n",
    "        best_params[name] = bayes_search.best_params_\n",
    "        clf = bayes_search.best_estimator_\n",
    "        print(f\"Melhores parâmetros para {name}: {bayes_search.best_params_}\\n\")\n",
    "\n",
    "#Optuna para RF e XGB\n",
    "\n",
    "# Funções de objetivo para Optuna\n",
    "def rf_objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 200)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 32, log=True)\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    return cross_val_score(clf, X_train_resampled, y_train_resampled, cv=kfold, scoring=\"accuracy\").mean()\n",
    "\n",
    "def xgb_objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "\n",
    "    # Encode the target variable to numeric values\n",
    "    le = LabelEncoder()\n",
    "    y_train_resampled_encoded = le.fit_transform(y_train_resampled)\n",
    "\n",
    "    clf = XGBClassifier(learning_rate=learning_rate, n_estimators=n_estimators,\n",
    "                        max_depth=max_depth, use_label_encoder=False, eval_metric='logloss')\n",
    "    return cross_val_score(clf, X_train_resampled, y_train_resampled_encoded, cv=kfold, scoring=\"accuracy\").mean()\n",
    "\n",
    "# Otimização com Optuna\n",
    "rf_study = optuna.create_study(direction=\"maximize\")\n",
    "rf_study.optimize(rf_objective, n_trials=5)\n",
    "classifiers['Random Forest'] = RandomForestClassifier(**rf_study.best_trial.params)\n",
    "print(\"Melhores hiperparâmetros para Random Forest:\", rf_study.best_trial.params)\n",
    "\n",
    "xgb_study = optuna.create_study(direction=\"maximize\")\n",
    "xgb_study.optimize(xgb_objective, n_trials=5)\n",
    "classifiers['XGBoost'] = XGBClassifier(**xgb_study.best_trial.params, use_label_encoder=False, eval_metric='logloss')\n",
    "print(\"Melhores hiperparâmetros para XGBoost:\", xgb_study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DNIIQajYi4I"
   },
   "source": [
    "## Resumo Melhores Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ULFB9uENYL2-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parâmetros do XGBoost sem valores None:\n",
      "{'objective': 'binary:logistic', 'enable_categorical': False, 'eval_metric': 'logloss', 'missing': nan, 'use_label_encoder': False}\n"
     ]
    }
   ],
   "source": [
    "# Obter os parâmetros do classificador e remover os que têm valor None\n",
    "xgb_clean_params = {k: v for k, v in classifiers['XGBoost'].get_params().items() if v is not None}\n",
    "\n",
    "# Criar um novo classificador com os parâmetros limpos\n",
    "cleaned_xgb = XGBClassifier(**xgb_clean_params)\n",
    "\n",
    "print(\"Parâmetros do XGBoost sem valores None:\")\n",
    "print(xgb_clean_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Y2Q3OKn7TAG9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classificadores com os melhores parâmetros:\n",
      "KNN: KNeighborsClassifier(n_neighbors=1, p=1)\n",
      "SVM: SVC(probability=True)\n",
      "Decision Tree: DecisionTreeClassifier(criterion='entropy', max_depth=36)\n",
      "LVQ: MLPClassifier(learning_rate_init=0.1, max_iter=100, solver='sgd')\n",
      "MLP: MLPClassifier(hidden_layer_sizes=(50,), max_iter=100)\n",
      "Ensemble Neural Network: VotingClassifier(estimators=[('mlp_relu',\n",
      "                              MLPClassifier(hidden_layer_sizes=(50, 20),\n",
      "                                            max_iter=100)),\n",
      "                             ('mlp_tanh',\n",
      "                              MLPClassifier(activation='tanh',\n",
      "                                            hidden_layer_sizes=(50, 20),\n",
      "                                            max_iter=100)),\n",
      "                             ('mlp_logistic',\n",
      "                              MLPClassifier(activation='logistic',\n",
      "                                            hidden_layer_sizes=(50, 20),\n",
      "                                            max_iter=100))],\n",
      "                 n_jobs=-1, voting='soft')\n",
      "Stacking: StackingClassifier(estimators=[('nb', GaussianNB()),\n",
      "                               ('dt', DecisionTreeClassifier(max_depth=3))],\n",
      "                   final_estimator=LogisticRegression(), n_jobs=-1)\n",
      "Random Forest: RandomForestClassifier(max_depth=28, n_estimators=97)\n",
      "XGBoost: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='logloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.09504317284612004, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=188, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)\n",
      "LightGBM: LGBMClassifier(max_depth=3, n_estimators=50)\n"
     ]
    }
   ],
   "source": [
    "# Como explodir um dicionário:\n",
    "# 'XGBoost': XGBClassifier(**xgb_study.best_trial.params),# Otimizado com Optuna\n",
    "\n",
    "# Converter parâmetros otimizados para inicializar os classificadores\n",
    "best_classifiers = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=1, p=1, weights='uniform'), # Otimizado com Bayesian Search\n",
    "    'SVM': svm,  # Nenhuma otimização aplicada\n",
    "    'Decision Tree': DecisionTreeClassifier(criterion='entropy', max_depth=36, min_samples_leaf=1, min_samples_split=2), # Otimizado com Bayesian Search\n",
    "    'LVQ': lvq,  # Nenhuma otimização aplicada\n",
    "    'MLP': mlp,  # Nenhuma otimização aplicada\n",
    "    'Ensemble Neural Network': ann_ensemble,  # Nenhuma otimização aplicada\n",
    "    'Stacking': stacking,  # Nenhuma otimização aplicada\n",
    "    'Random Forest': RandomForestClassifier(max_depth=28, n_estimators=97), # Otimizado com Optuna    \n",
    "    'XGBoost': XGBClassifier(objective='binary:logistic', enable_categorical=False, eval_metric='logloss', \n",
    "                             learning_rate=0.09504317284612004, max_depth=6, n_estimators=188),  # Otimizado com Optuna\n",
    "    'LightGBM': lgbm # Nenhuma otimização aplicada\n",
    "}\n",
    "\n",
    "# Mostrar os melhores parâmetros para os classificadores otimizados\n",
    "print(\"Classificadores com os melhores parâmetros:\")\n",
    "\n",
    "for name, model in best_classifiers.items():\n",
    "  print(f\"{name}: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsvC5TygOx46"
   },
   "source": [
    "## Avaliando os modelos após Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BhWSXIJJO2uv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN - Accuracy: 0.9586, F1 Score: 0.9572, Recall: 0.9179, ACSA: 0.9590\n",
      "Confusion Matrix:\n",
      "[[7821    0]\n",
      " [ 479 7488]]\n",
      "\n",
      "SVM - Accuracy: 0.9680, F1 Score: 0.9673, Recall: 0.9386, ACSA: 0.9683\n",
      "Confusion Matrix:\n",
      "[[7814    7]\n",
      " [ 461 7506]]\n",
      "\n",
      "Decision Tree - Accuracy: 0.9839, F1 Score: 0.9840, Recall: 0.9812, ACSA: 0.9839\n",
      "Confusion Matrix:\n",
      "[[7821    0]\n",
      " [   0 7967]]\n",
      "\n",
      "LVQ - Accuracy: 0.9936, F1 Score: 0.9936, Recall: 0.9903, ACSA: 0.9936\n",
      "Confusion Matrix:\n",
      "[[7807   14]\n",
      " [   3 7964]]\n",
      "\n",
      "MLP - Accuracy: 0.9895, F1 Score: 0.9895, Recall: 0.9828, ACSA: 0.9895\n",
      "Confusion Matrix:\n",
      "[[7811   10]\n",
      " [  45 7922]]\n",
      "\n",
      "Ensemble Neural Network - Accuracy: 0.9949, F1 Score: 0.9950, Recall: 0.9915, ACSA: 0.9950\n",
      "Confusion Matrix:\n",
      "[[7821    0]\n",
      " [  25 7942]]\n",
      "\n",
      "Stacking - Accuracy: 0.8867, F1 Score: 0.8892, Recall: 0.9030, ACSA: 0.8865\n",
      "Confusion Matrix:\n",
      "[[6682 1139]\n",
      " [ 598 7369]]\n",
      "\n",
      "Random Forest - Accuracy: 0.9912, F1 Score: 0.9912, Recall: 0.9864, ACSA: 0.9912\n",
      "Confusion Matrix:\n",
      "[[7821    0]\n",
      " [   0 7967]]\n",
      "\n",
      "XGBoost - Accuracy: 0.9915, F1 Score: 0.9915, Recall: 0.9869, ACSA: 0.9916\n",
      "Confusion Matrix:\n",
      "[[7821    0]\n",
      " [   0 7967]]\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 7170, number of negative: 7039\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5703\n",
      "[LightGBM] [Info] Number of data points in the train set: 14209, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504610 -> initscore=0.018440\n",
      "[LightGBM] [Info] Start training from score 0.018440\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7170, number of negative: 7039\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5700\n",
      "[LightGBM] [Info] Number of data points in the train set: 14209, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504610 -> initscore=0.018440\n",
      "[LightGBM] [Info] Start training from score 0.018440\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7170, number of negative: 7039\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5710\n",
      "[LightGBM] [Info] Number of data points in the train set: 14209, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504610 -> initscore=0.018440\n",
      "[LightGBM] [Info] Start training from score 0.018440\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7170, number of negative: 7039\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5712\n",
      "[LightGBM] [Info] Number of data points in the train set: 14209, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504610 -> initscore=0.018440\n",
      "[LightGBM] [Info] Start training from score 0.018440\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7170, number of negative: 7039\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5701\n",
      "[LightGBM] [Info] Number of data points in the train set: 14209, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504610 -> initscore=0.018440\n",
      "[LightGBM] [Info] Start training from score 0.018440\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7170, number of negative: 7039\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5707\n",
      "[LightGBM] [Info] Number of data points in the train set: 14209, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504610 -> initscore=0.018440\n",
      "[LightGBM] [Info] Start training from score 0.018440\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7170, number of negative: 7039\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5572\n",
      "[LightGBM] [Info] Number of data points in the train set: 14209, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504610 -> initscore=0.018440\n",
      "[LightGBM] [Info] Start training from score 0.018440\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7171, number of negative: 7038\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5588\n",
      "[LightGBM] [Info] Number of data points in the train set: 14209, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504680 -> initscore=0.018721\n",
      "[LightGBM] [Info] Start training from score 0.018721\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7171, number of negative: 7039\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5691\n",
      "[LightGBM] [Info] Number of data points in the train set: 14210, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504645 -> initscore=0.018579\n",
      "[LightGBM] [Info] Start training from score 0.018579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7171, number of negative: 7039\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5682\n",
      "[LightGBM] [Info] Number of data points in the train set: 14210, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504645 -> initscore=0.018579\n",
      "[LightGBM] [Info] Start training from score 0.018579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 7967, number of negative: 7821\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5861\n",
      "[LightGBM] [Info] Number of data points in the train set: 15788, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.504624 -> initscore=0.018496\n",
      "[LightGBM] [Info] Start training from score 0.018496\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM - Accuracy: 0.9457, F1 Score: 0.9442, Recall: 0.9101, ACSA: 0.9461\n",
      "Confusion Matrix:\n",
      "[[7712  109]\n",
      " [ 709 7258]]\n",
      "\n",
      "\n",
      "Resumo dos melhores parâmetros:\n",
      "KNN: OrderedDict([('n_neighbors', 1), ('p', 1), ('weights', 'uniform')])\n",
      "Decision Tree: OrderedDict([('criterion', 'entropy'), ('max_depth', 38), ('min_samples_leaf', 1), ('min_samples_split', 2)])\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Filtrar warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def calculate_acsa(y_true, y_pred):\n",
    "    classes = np.unique(y_true)\n",
    "    class_accuracies = [\n",
    "        accuracy_score(y_true[y_true == c], y_pred[y_true == c]) for c in classes\n",
    "    ]\n",
    "    return np.mean(class_accuracies)\n",
    "\n",
    "def evaluate_model(model, X, y, kfold):\n",
    "    # Converter para array numpy se for DataFrame/Series\n",
    "    X_numpy = X.values if hasattr(X, 'values') else X\n",
    "    y_numpy = y.values if hasattr(y, 'values') else y\n",
    "    \n",
    "    # Usar cross_validate para calcular accuracy, f1 e recall em uma única chamada\n",
    "    scoring = ['accuracy', 'f1', 'recall']\n",
    "    scores = cross_validate(model, X_numpy, y_numpy, cv=kfold, scoring=scoring, return_estimator=True)\n",
    "\n",
    "    # Métricas médias da validação cruzada\n",
    "    accuracy = scores['test_accuracy'].mean()\n",
    "    f1 = scores['test_f1'].mean()\n",
    "    recall = scores['test_recall'].mean()\n",
    "\n",
    "    # Cálculo do ACSA manualmente\n",
    "    acsa_scores = []\n",
    "    for estimator, (train_idx, val_idx) in zip(scores['estimator'], kfold.split(X_numpy, y_numpy)):\n",
    "        X_val_fold = X_numpy[val_idx]\n",
    "        y_val_fold = y_numpy[val_idx]\n",
    "        y_pred_fold = estimator.predict(X_val_fold)\n",
    "        acsa_scores.append(calculate_acsa(y_val_fold, y_pred_fold))\n",
    "    acsa = np.mean(acsa_scores)\n",
    "\n",
    "    # Ajustar modelo nos dados completos e calcular a matriz de confusão\n",
    "    model.fit(X_numpy, y_numpy)\n",
    "    y_pred = model.predict(X_numpy)\n",
    "    conf_matrix = confusion_matrix(y_numpy, y_pred)\n",
    "\n",
    "    return accuracy, f1, recall, acsa, conf_matrix\n",
    "\n",
    "# Uso do código\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    # Avaliar desempenho do classificador\n",
    "    accuracy, f1, recall, acsa, conf_matrix = evaluate_model(clf, X_train_resampled, y_train_resampled, kfold)\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'ACSA': acsa,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Recall: {recall:.4f}, ACSA: {acsa:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
    "\n",
    "# Resultados\n",
    "print(\"\\nResumo dos melhores parâmetros:\")\n",
    "for name, params in best_params.items():\n",
    "    print(f\"{name}: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGeDInlGjm2L"
   },
   "source": [
    "## Amostra para análise Estatística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uTcxxjHKk6Oo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando KNN...\n",
      "\n",
      "Avaliando SVM...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 121\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, clf \u001b[38;5;129;01min\u001b[39;00m classifiers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAvaliando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m     results[name] \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Imprimindo resultados\u001b[39;00m\n\u001b[0;32m    130\u001b[0m print_results(results)\n",
      "Cell \u001b[1;32mIn[12], line 49\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, X, y, kfold, n_samples)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;66;03m# Realizando a validação cruzada\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43my_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Usar todos os cores disponíveis\u001b[39;49;00m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;66;03m# Métricas médias da validação cruzada\u001b[39;00m\n\u001b[0;32m     60\u001b[0m         results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[1;32mc:\\Users\\willi\\Documents\\Projetos\\mestrado_mineracao\\mda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\willi\\Documents\\Projetos\\mestrado_mineracao\\mda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\willi\\Documents\\Projetos\\mestrado_mineracao\\mda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\willi\\Documents\\Projetos\\mestrado_mineracao\\mda\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\willi\\Documents\\Projetos\\mestrado_mineracao\\mda\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\willi\\Documents\\Projetos\\mestrado_mineracao\\mda\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from xgboost import set_config\n",
    "\n",
    "# Filtrar warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('sklearn').setLevel(logging.ERROR)\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "set_config(verbosity=0)\n",
    "\n",
    "def calculate_acsa(y_true, y_pred):\n",
    "    \"\"\"Calcula o Average Class-Specific Accuracy (ACSA).\"\"\"\n",
    "    classes = np.unique(y_true)\n",
    "    class_accuracies = [\n",
    "        accuracy_score(y_true[y_true == c], y_pred[y_true == c]) for c in classes\n",
    "    ]\n",
    "    return np.mean(class_accuracies)\n",
    "\n",
    "def evaluate_model(model, X, y, kfold, n_samples=10):\n",
    "    \"\"\"\n",
    "    Avalia o modelo usando validação cruzada múltiplas vezes.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo de classificação\n",
    "        X: Features (DataFrame ou array)\n",
    "        y: Target (Series ou array)\n",
    "        kfold: Objeto de validação cruzada\n",
    "        n_samples: Número de iterações de avaliação\n",
    "    \"\"\"\n",
    "    # Converter para numpy arrays se necessário\n",
    "    X_numpy = X.values if hasattr(X, 'values') else np.array(X)\n",
    "    y_numpy = y.values if hasattr(y, 'values') else np.array(y)\n",
    "    \n",
    "    results = {\n",
    "        'Accuracy': [],\n",
    "        'F1 Score': [],\n",
    "        'Recall': [],\n",
    "        'ACSA': [],\n",
    "        'Confusion Matrix': []\n",
    "    }\n",
    "    \n",
    "    for sample in range(n_samples):\n",
    "        try:\n",
    "            # Realizando a validação cruzada\n",
    "            scores = cross_validate(\n",
    "                model, \n",
    "                X_numpy, \n",
    "                y_numpy, \n",
    "                cv=kfold, \n",
    "                scoring=['accuracy', 'f1', 'recall'],\n",
    "                return_estimator=True,\n",
    "                n_jobs=-1  # Usar todos os cores disponíveis\n",
    "            )\n",
    "\n",
    "            # Métricas médias da validação cruzada\n",
    "            results['Accuracy'].append(scores['test_accuracy'].mean())\n",
    "            results['F1 Score'].append(scores['test_f1'].mean())\n",
    "            results['Recall'].append(scores['test_recall'].mean())\n",
    "\n",
    "            # Cálculo do ACSA\n",
    "            acsa_scores = []\n",
    "            for estimator, (_, val_idx) in zip(scores['estimator'], kfold.split(X_numpy, y_numpy)):\n",
    "                X_val = X_numpy[val_idx]\n",
    "                y_val = y_numpy[val_idx]\n",
    "                y_pred = estimator.predict(X_val)\n",
    "                acsa_scores.append(calculate_acsa(y_val, y_pred))\n",
    "            \n",
    "            results['ACSA'].append(np.mean(acsa_scores))\n",
    "\n",
    "            # Matriz de confusão usando o último modelo ajustado\n",
    "            model.fit(X_numpy, y_numpy)\n",
    "            y_pred_final = model.predict(X_numpy)\n",
    "            results['Confusion Matrix'].append(confusion_matrix(y_numpy, y_pred_final))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na amostra {sample}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return results\n",
    "\n",
    "def print_results(results_dict):\n",
    "    \"\"\"Imprime os resultados de forma organizada.\"\"\"\n",
    "    for name, metrics in results_dict.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Resultados para {name}:\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Métricas numéricas\n",
    "        for metric in ['Accuracy', 'F1 Score', 'Recall', 'ACSA']:\n",
    "            values = metrics[metric]\n",
    "            if values:  # Verifica se há valores\n",
    "                mean = np.mean(values)\n",
    "                std = np.std(values)\n",
    "                print(f\"\\n{metric}:\")\n",
    "                print(f\"  Média: {mean:.4f}\")\n",
    "                print(f\"  Desvio Padrão: {std:.4f}\")\n",
    "                print(f\"  95% IC: [{mean - 1.96*std/np.sqrt(len(values)):.4f}, \"\n",
    "                      f\"{mean + 1.96*std/np.sqrt(len(values)):.4f}]\")\n",
    "        \n",
    "        # Matriz de Confusão\n",
    "        print(\"\\nMatriz de Confusão (média):\")\n",
    "        if metrics['Confusion Matrix']:\n",
    "            mean_conf_matrix = np.mean(metrics['Confusion Matrix'], axis=0)\n",
    "            print(mean_conf_matrix.astype(int))\n",
    "\n",
    "# Uso do código\n",
    "if __name__ == \"__main__\":\n",
    "    # Definindo a validação cruzada\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Inicializando resultados\n",
    "    results = {}\n",
    "    \n",
    "    # Avaliando cada classificador\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\nAvaliando {name}...\")\n",
    "        results[name] = evaluate_model(\n",
    "            clf, \n",
    "            X_train_resampled, \n",
    "            y_train_resampled, \n",
    "            kfold, \n",
    "            n_samples=10\n",
    "        )\n",
    "    \n",
    "    # Imprimindo resultados\n",
    "    print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht5NaQt7eJ-C"
   },
   "source": [
    "# Roteiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBsbCN7EdMOY"
   },
   "source": [
    "- Registro dos resultados em **dataframe**\n",
    "- Visualização de Resultados: Gráficos de caixa (boxplot), barras, etc.\n",
    "- Análise Estatística: Relatório de classificação e testes de significância estatística.\n",
    "- Aplicar o protocolo utilizado de comparação de classificadores de Janez Demsar.\n",
    "- Apresentar desempenhos de treinamento e teste para todos os modelos.\n",
    "- Análise de Custo-Benefício: Avaliar os recursos computacionais utilizados por cada modelo (tempo de processamento e memória) em relação ao desempenho alcançado.\n",
    "- Teste de Estresse dos Modelos: Realizar testes com dados novos e desconhecidos para avaliar a robustez dos modelos.\n",
    "- Métricas de Complexidade do Modelo: Avaliar a complexidade dos modelos, como número de parâmetros e tempo de inferência.\n",
    "- Explainable AI (XAI): Explicar as previsões de pelo menos dois dos modelos utilizando ferramentas de XAI, como SHAP ou LIME, para aumentar a compreensão sobre os fatores que influenciam as decisões dos modelos. Escolher o melhor modelo a partir da comparação estatística realizada para aplicar o XAI."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "TqbQfcJwluv-",
    "Xz2A65H1DnfE",
    "2qC30mhSDt27"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
